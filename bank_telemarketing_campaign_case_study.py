# -*- coding: utf-8 -*-
"""Bank Telemarketing Campaign Case Study

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CVWZSgQxdue44vnERmLtT90_dXBMW0L5
"""



"""##### Bank Telemarketing Campaign Case Study.

#### Problem Statement:
The bank provides financial services/products such as savings accounts, current accounts, debit cards, etc. to its customers. In order to increase its overall revenue, the bank conducts various marketing campaigns for its financial products such as credit cards, term deposits, loans, etc. These campaigns are intended for the bank’s existing customers. However, the marketing campaigns need to be cost-efficient so that the bank not only increases their overall revenues but also the total profit. You need to apply your knowledge of EDA on the given dataset to analyse the patterns and provide inferences/solutions for the future marketing campaign.

The bank conducted a telemarketing campaign for one of its financial products ‘Term Deposits’ to help foster long-term relationships with existing customers. The dataset contains information about all the customers who were contacted during a particular year to open term deposit accounts.


**What is the term Deposit?**

Term deposits also called fixed deposits, are the cash investments made for a specific time period ranging from 1 month to 5 years for predetermined fixed interest rates. The fixed interest rates offered for term deposits are higher than the regular interest rates for savings accounts. The customers receive the total amount (investment plus the interest) at the end of the maturity period. Also, the money can only be withdrawn at the end of the maturity period. Withdrawing money before that will result in an added penalty associated, and the customer will not receive any interest returns.

Your target is to do end to end EDA on this bank telemarketing campaign data set to infer knowledge that where bank has to put more effort to improve it's positive response rate.


"""

#import libraries and warings
import warnings
warnings.filterwarnings("ignore")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""### Segment- 2, Data Types
There are multiple types of data types available in the data set. some of them are numerical type and some of categorical type. You are required to get the idea about the data types after reading the data frame.

Following are the some of the types of variables:
- **Numeric data type**: banking dataset: salary, balance, duration and age.
- **Categorical data type**: banking dataset: education, job, marital, poutcome and month etc.
- **Ordinal data type**: banking dataset: Age group.
- **Time and date type**
- **Coordinates type of data**: latitude and longitude type.
"""

df =  pd.read_csv("/content/bank_marketing_updated_v1.csv", skiprows = 2)

df.head()

"""### Segment- 3, Fixing the Rows and Columns
Checklist for fixing rows:
- **Delete summary rows**: Total and Subtotal rows
- **Delete incorrect rows**: Header row and footer row
- **Delete extra rows**: Column number, indicators, Blank rows, Page No.

Checklist for fixing columns:
- **Merge columns for creating unique identifiers**, if needed, for example, merge the columns State and City into the column Full address.
- **Split columns to get more data**: Split the Address column to get State and City columns to analyse each separately.
- **Add column names**: Add column names if missing.
- **Rename columns consistently**: Abbreviations, encoded columns.
- **Delete columns**: Delete unnecessary columns.
- **Align misaligned columns**: The data set may have shifted columns, which you need to align correctly.
"""

#droping customer id
df.drop('customerid', axis = 1 , inplace = True)
df.head()

#split jobedu into 2 column
df['job'] =  df['jobedu'].str.split(',').str.get(0)
df['education'] =  df['jobedu'].str.split(',').str.get(1)

#droping jobedu column
df.drop('jobedu', axis =1, inplace = True)

df.head()

#checking the missing value
df.isnull().sum()

df.shape

#checking the missing value in age colum
df.age.isnull().sum()

#checking the percentage
100*19/45211

#droping missing vlaue
df = df.dropna(subset = 'age')

#handling missing value of month
df['month'].isnull().sum()

df['month'].value_counts(normalize=True)

#filling value with mode
df.isnull().sum()

month_moode = df['month'].mode()[0]
month_mode

df['month'].fillna('may, 2017', inplace =  True)

df['month'].isnull().sum()

#checking the pdays
df['pdays'].value_counts()

df['pdays'].describe()

"""-1 indicates the missing values.
Missing value does not always be present as null.
How to handle it:

Objective is:
- you should ignore the missing values in the calculations
- simply make it missing - replace -1 with NaN.
- all summary statistics- mean, median etc. we will ignore the missing values of pdays.
"""

df.loc[df.pdays<0,'pdays'] = np.NaN

df['pdays'].describe()

"""#outlier handling"""

df['age'].describe()

df['age'].plot.hist()
plt.show()

sns.boxplot(df['age'])
plt.show()

#balace variable
df['balance'].describe()

plt.figure(figsize = [8,8])
sns.boxplot(df['balance'])
plt.show()

#standardize variable
df['duration'].head()

df['duration'].describe()

#some value are in second and some are in minute we have to fix it
df['duration'] = df['duration'].apply(lambda x : float(x.split()[0])/60 if x.find('sec')>0 else float(x.split()[0]))

df['duration'].describe()

df.info()

"""###univarite analysis of catagorical data

Categorical data type: banking dataset: education, job, marital, poutcome and month etc.
"""

df['marital'].value_counts()

df['marital'].value_counts(normalize = True)

df['marital'].value_counts(normalize = True).plot.bar()
plt.show()

"""As we can see more member comes from married cat."""

df['education'].value_counts()

df['education'].value_counts(normalize = True).plot.bar()
plt.show()

df['job'].value_counts()

df['job'].value_counts(normalize = True).plot.bar()
plt.show()

df.head()

df['poutcome'].value_counts()

df['poutcome'].value_counts(normalize = True).plot.bar()

#target varible
df['response'].value_counts(normalize =True)

df['response'].value_counts(normalize =True).plot.pie(autopct='%1.1f%%')
plt.show()

"""Bivariate Analysis

numerical to numerical
"""

plt.scatter(df['salary'], df['balance'])
plt.show()

df.plot.scatter(x = 'age', y = 'balance')

sns.pairplot(data = df , vars = ['salary','balance','age'])
plt.show()

"""quantify using coreelation"""

df[['age','balance','salary']].corr()

#corelation heatmap
sns.heatmap(df[['age','balance','salary']].corr(), annot = True, cmap = 'Reds')

df.groupby('response')['salary'].mean()

df.groupby('response')['salary'].median()

sns.boxplot(data = df, x = 'response', y = 'salary')

sns.boxplot(data = df, x = 'response', y = 'balance')

df.groupby('response')['balance'].mean()

df.groupby('response')['balance'].median()

def p75 (x):
  return np.quantile(x, 0.75)

df.groupby('response')['balance'].aggregate(['mean','median', p75])

df.groupby('response')['balance'].aggregate(['mean','median']).plot.bar()
plt.show()

"""##cat to cat"""

df['response_flag'] = np.where(df['response'] == 'yes',1,0)

df['response_flag'].value_counts()

df['response'].value_counts()

df['response_flag'].value_counts(normalize = True)

df['response_flag'].mean()

"""##education and response rate"""

df.groupby(['education'])['response_flag'].mean()

"""we can education var is ha effective of yrs resonses more



##marital and response rate
"""

df.groupby(['marital'])['response_flag'].mean()

df.groupby(['marital'])['response_flag'].mean().plot.bar()
 plt.show()

"""as we can se we have to more focus on sigle guy according to our data



##loand and response rate
"""

df.groupby(['loan'])['response_flag'].mean().plot.bar()
 plt.show()

"""people have already loan they are not convering to the yes response yes this naturally nd its obvious


##housing and response rate
"""

df.groupby(['housing'])['response_flag'].mean().plot.bar()
 plt.show()

"""people who already take take home loan they are not responding to camp


##age vs response
"""

sns.boxplot(data =  df , x = 'response', y = 'age')
plt.show()

"""#making bucket from the age colum"""

pd.cut(df['age'][: 5],[0,30,40,50,60,999], labels = ['<30','30 - 40','40 - 50', '50-60', '60+'])

df['age_group'] = pd.cut(df['age'],[0,30,40,50,60,999], labels = ['<30','30 - 40','40 - 50', '50-60', '60+'])

df['age_group'].value_counts()

plt.figure(figsize = (10,4))
plt.subplot(1,2,1)
df['age_group'].value_counts(normalize = True).plot.bar()
plt.subplot(1,2,2)
df.groupby(['age_group'])['response_flag'].mean().plot.bar()
plt.show()

"""as we see greater than 40 has more positve response."""

df.groupby(['job'])['response_flag'].mean().plot.bar()
plt.show()

"""##multivariate analysis"""

res = pd.pivot_table(data = df, index = 'education', columns ="marital", values= 'response_flag')
res

sns.heatmap(res, annot = True)

"""#jobs vs marital vs response"""

res = pd.pivot_table(data = df, index = 'job', columns ="marital", values= 'response_flag')
sns.heatmap(res, annot = True)

"""#education vs potcome  vs response"""

res = pd.pivot_table(data = df, index = 'education', columns ="poutcome", values= 'response_flag')
sns.heatmap(res, annot = True)

